{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributors\n",
    "AmirMohammad Bandari (401110278) & Pouria Mahmoudkhan (401110289)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Preparing Data & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "downloading data in `./data` folder and splitting it into `train`, `validation`, and `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data_root = './data'\n",
    "train_full = datasets.FashionMNIST(root=data_root, train=True, download=True, transform=transform)\n",
    "test_ds = datasets.FashionMNIST(root=data_root, train=False, download=True, transform=transform)\n",
    "\n",
    "n_train = int(0.9 * len(train_full))\n",
    "n_val = len(train_full) - n_train\n",
    "train_ds, val_ds = random_split(train_full, [n_train, n_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds), train_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity: show a batch 25 images with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_full.classes\n",
    "x, y = next(iter(train_loader))\n",
    "x = x[:25]\n",
    "y = y[:25]\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(x[i].squeeze(0), cmap='gray')\n",
    "    plt.title(classes[int(y[i])], fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA: class distribution in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [int(train_full.targets[i]) for i in train_ds.indices]\n",
    "cnt = Counter(train_labels)\n",
    "xs = np.arange(10)\n",
    "vals = np.array([cnt[i] for i in xs])\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.bar(xs, vals)\n",
    "plt.xticks(xs, classes, rotation=45, ha='right')\n",
    "plt.ylabel('count')\n",
    "plt.title('Train class distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel intensity histogram (sampled for speed)\n",
    "def sample_pixels(loader, max_batches=80):\n",
    "    px = []\n",
    "    for i, (x, _) in enumerate(loader):\n",
    "        px.append(x.view(-1).cpu().numpy())\n",
    "        if i+1 >= max_batches:\n",
    "            break\n",
    "    px = np.concatenate(px, axis=0)\n",
    "    return px\n",
    "\n",
    "px = sample_pixels(train_loader, max_batches=60)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(px, bins=50)\n",
    "plt.title('Pixel intensity histogram (train, sampled)')\n",
    "plt.xlabel('intensity')\n",
    "plt.ylabel('freq')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image-level histogram: mean pixel intensity per image (sampled)\n",
    "means = []\n",
    "for i, (x, _) in enumerate(train_loader):\n",
    "    means.append(x.view(x.size(0), -1).mean(dim=1).cpu().numpy())\n",
    "    if i >= 80:\n",
    "        break\n",
    "means = np.concatenate(means, axis=0)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(means, bins=50)\n",
    "plt.title('Per-image mean intensity (train, sampled)')\n",
    "plt.xlabel('mean intensity')\n",
    "plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n",
    "here we are defining some helper functions:\n",
    "- `to_img_grid` gets a tensor in input and view it as a grid of images, it helps in demonstrations\n",
    "- `evaluate_vae` abstracts away the of evaluation process of our models (they all use a same function)\n",
    "- `train_vae` abstracts away the training process of our models (they all use a same function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img_grid(x, nrow, ncol, figsize=(10,4), title=None):\n",
    "    x = x.detach().cpu()\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(nrow*ncol):\n",
    "        plt.subplot(nrow, ncol, i+1)\n",
    "        plt.imshow(x[i].squeeze(0), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    if title is not None:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_vae(model, loader, beta=1.0):\n",
    "    model.eval()\n",
    "    rec_sum, kld_sum, total_sum, n = 0.0, 0.0, 0.0, 0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        out = model(x)\n",
    "        x_hat, mu, logvar = out[\"x_hat\"], out[\"mu\"], out[\"logvar\"]\n",
    "        rec = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        total = rec + beta * kld\n",
    "        rec_sum += rec.item()\n",
    "        kld_sum += kld.item()\n",
    "        total_sum += total.item()\n",
    "        n += x.size(0)\n",
    "    return {\n",
    "        \"recon_per_img\": rec_sum / n,\n",
    "        \"kld_per_img\": kld_sum / n,\n",
    "        \"total_per_img\": total_sum / n\n",
    "    }\n",
    "\n",
    "def train_vae(model, train_loader, val_loader, epochs=10, lr=2e-3, beta=1.0, warmup_epochs=0):\n",
    "    model.to(device)\n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "    hist = {\"train_total\":[], \"val_total\":[], \"train_rec\":[], \"train_kld\":[], \"val_rec\":[], \"val_kld\":[]}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        rec_sum, kld_sum, tot_sum, n = 0.0, 0.0, 0.0, 0\n",
    "        if warmup_epochs > 0:\n",
    "            beta_ep = beta * min(1.0, ep / warmup_epochs)\n",
    "        else:\n",
    "            beta_ep = beta\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            x_hat, mu, logvar = out[\"x_hat\"], out[\"mu\"], out[\"logvar\"]\n",
    "            rec = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "            kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = rec + beta_ep * kld\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            rec_sum += rec.item()\n",
    "            kld_sum += kld.item()\n",
    "            tot_sum += loss.item()\n",
    "            n += x.size(0)\n",
    "\n",
    "        tr = {\"recon_per_img\": rec_sum/n, \"kld_per_img\": kld_sum/n, \"total_per_img\": tot_sum/n}\n",
    "        va = evaluate_vae(model, val_loader, beta=beta)\n",
    "        hist[\"train_total\"].append(tr[\"total_per_img\"])\n",
    "        hist[\"val_total\"].append(va[\"total_per_img\"])\n",
    "        hist[\"train_rec\"].append(tr[\"recon_per_img\"])\n",
    "        hist[\"train_kld\"].append(tr[\"kld_per_img\"])\n",
    "        hist[\"val_rec\"].append(va[\"recon_per_img\"])\n",
    "        hist[\"val_kld\"].append(va[\"kld_per_img\"])\n",
    "        print(f\"ep {ep:02d} | beta={beta_ep:.3f} | train total {tr['total_per_img']:.2f} rec {tr['recon_per_img']:.2f} kld {tr['kld_per_img']:.2f} | val total {va['total_per_img']:.2f}\")\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: VAE implementation & quality improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define VAE Model\n",
    "\n",
    "We define our base model which has a fully connected architecture, a Multi-Layer Perceptron.\n",
    "We suppose our latent dimension to be 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPVAE(nn.Module):\n",
    "    def __init__(self, z_dim=20, h_dim=400):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, h_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu = nn.Linear(h_dim, z_dim)\n",
    "        self.logvar = nn.Linear(h_dim, z_dim)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.enc(x)\n",
    "        mu = self.mu(h)\n",
    "        logvar = self.logvar(h)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        x_hat = self.dec(z).view(-1, 1, 28, 28)\n",
    "        return {\"x_hat\": x_hat, \"mu\": mu, \"logvar\": logvar, \"z\": z}\n",
    "\n",
    "@torch.no_grad()\n",
    "def recon_samples(model, loader, n=20):\n",
    "    model.eval()\n",
    "    x, _ = next(iter(loader))\n",
    "    x = x[:n].to(device)\n",
    "    out = model(x)\n",
    "    return x, out[\"x_hat\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_prior(model, n=50):\n",
    "    model.eval()\n",
    "    z = torch.randn(n, model.z_dim).to(device)\n",
    "    x_hat = model.dec(z).view(-1, 1, 28, 28)\n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_base = MLPVAE(z_dim=20, h_dim=400).to(device)\n",
    "hist_base = train_vae(vae_base, train_loader, val_loader, epochs=10, lr=2e-3, beta=1.0)\n",
    "\n",
    "base_test = evaluate_vae(vae_base, test_loader, beta=1.0)\n",
    "base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructions (20)\n",
    "x_in, x_out = recon_samples(vae_base, test_loader, n=20)\n",
    "to_img_grid(x_in, 2, 10, figsize=(12,3), title=\"Baseline: inputs\")\n",
    "to_img_grid(x_out, 2, 10, figsize=(12,3), title=\"Baseline: reconstructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling (50)\n",
    "samp = sample_prior(vae_base, n=50)\n",
    "to_img_grid(samp, 5, 10, figsize=(12,6), title=\"Baseline: prior samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our improved architecture.\n",
    "This new version has a convolution based structure.\n",
    "- First a layer of 32-channeled 4x4 convolution with stride 2 and padding 1.\n",
    "- Then a layer of 64-channeled 4x4 convolution with stride 2 and padding 1.\n",
    "- Then finally a 128-channeled 3x3 convolution with stride and padding 1.\n",
    "\n",
    "Leaving us with 128 channels with size 7x7.\n",
    "This enters a linear perceptron layer.\n",
    "\n",
    "The decoder is built to be the inverse of this structure.\n",
    "\n",
    "Because this structure uses our inductive bias, it will probably give a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.enc_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1),  # 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), # 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),# 7x7\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.enc_fc = nn.Linear(128*7*7, 256)\n",
    "        self.mu = nn.Linear(256, z_dim)\n",
    "        self.logvar = nn.Linear(256, z_dim)\n",
    "\n",
    "        self.dec_fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128*7*7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec_deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), # 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # 28x28\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 3, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.enc_conv(x).view(x.size(0), -1)\n",
    "        h = F.relu(self.enc_fc(h))\n",
    "        mu = self.mu(h)\n",
    "        logvar = self.logvar(h)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        g = self.dec_fc(z).view(-1, 128, 7, 7)\n",
    "        x_hat = self.dec_deconv(g)\n",
    "        return {\"x_hat\": x_hat, \"mu\": mu, \"logvar\": logvar, \"z\": z}\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_prior_conv(model, n=50):\n",
    "    model.eval()\n",
    "    z = torch.randn(n, model.z_dim).to(device)\n",
    "    g = model.dec_fc(z).view(-1, 128, 7, 7)\n",
    "    x_hat = model.dec_deconv(g)\n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_imp = ConvVAE(z_dim=32).to(device)\n",
    "hist_imp = train_vae(vae_imp, train_loader, val_loader, epochs=12, lr=2e-3, beta=1.0, warmup_epochs=6)\n",
    "\n",
    "imp_test = evaluate_vae(vae_imp, test_loader, beta=1.0)\n",
    "imp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before vs after (reconstruction)\n",
    "x_in, x_out_base = recon_samples(vae_base, test_loader, n=20)\n",
    "_, x_out_imp = recon_samples(vae_imp, test_loader, n=20)\n",
    "\n",
    "to_img_grid(x_out_base, 2, 10, figsize=(12,3), title=\"Baseline reconstructions\")\n",
    "to_img_grid(x_out_imp, 2, 10, figsize=(12,3), title=\"Improved reconstructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before vs after (sampling)\n",
    "samp_base = sample_prior(vae_base, n=50)\n",
    "samp_imp = sample_prior_conv(vae_imp, n=50)\n",
    "to_img_grid(samp_base, 5, 10, figsize=(12,6), title=\"Baseline samples\")\n",
    "to_img_grid(samp_imp, 5, 10, figsize=(12,6), title=\"Improved samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now examine our test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(rows, headers):\n",
    "    w = [max(len(h), max(len(str(r[i])) for r in rows)) for i, h in enumerate(headers)]\n",
    "    line = \" | \".join(h.ljust(w[i]) for i, h in enumerate(headers))\n",
    "    sep = \"-+-\".join(\"-\"*w[i] for i in range(len(headers)))\n",
    "    print(line)\n",
    "    print(sep)\n",
    "    for r in rows:\n",
    "        print(\" | \".join(str(r[i]).ljust(w[i]) for i in range(len(headers))))\n",
    "\n",
    "rows = [\n",
    "    [\"MLP VAE (base)\", f\"{base_test['recon_per_img']:.2f}\", f\"{base_test['kld_per_img']:.2f}\", f\"{base_test['total_per_img']:.2f}\"],\n",
    "    [\"Conv VAE (imp)\", f\"{imp_test['recon_per_img']:.2f}\", f\"{imp_test['kld_per_img']:.2f}\", f\"{imp_test['total_per_img']:.2f}\"],\n",
    "]\n",
    "print_table(rows, [\"model\", \"recon/img\", \"kld/img\", \"total/img\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Controlling latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, by changing the regularization parameter beta, we search for the optimal hyperparameter.\n",
    "First we define a function for our latent traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def latent_traversal(model, x, dims, steps=7, span=3.0):\n",
    "    model.eval()\n",
    "    x = x.to(device)\n",
    "    out = model(x)\n",
    "    mu = out[\"mu\"][0]\n",
    "    z0 = mu.clone()\n",
    "\n",
    "    vals = torch.linspace(-span, span, steps, device=device)\n",
    "    grids = []\n",
    "    for d in dims:\n",
    "        zs = []\n",
    "        for v in vals:\n",
    "            z = z0.clone()\n",
    "            z[d] = v\n",
    "            zs.append(z.unsqueeze(0))\n",
    "        zs = torch.cat(zs, dim=0)\n",
    "        if isinstance(model, ConvVAE):\n",
    "            g = model.dec_fc(zs).view(-1, 128, 7, 7)\n",
    "            x_hat = model.dec_deconv(g)\n",
    "        else:\n",
    "            x_hat = model.dec(zs).view(-1, 1, 28, 28)\n",
    "        grids.append(x_hat)\n",
    "    return grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [0.5, 1.0, 4.0]\n",
    "beta_models = {}\n",
    "beta_tests = {}\n",
    "\n",
    "for b in betas:\n",
    "    m = ConvVAE(z_dim=32).to(device)\n",
    "    _ = train_vae(m, train_loader, val_loader, epochs=8, lr=2e-3, beta=b, warmup_epochs=4)\n",
    "    beta_models[b] = m\n",
    "    beta_tests[b] = evaluate_vae(m, test_loader, beta=b)\n",
    "\n",
    "rows = [[f\"beta={b}\", f\"{beta_tests[b]['recon_per_img']:.2f}\", f\"{beta_tests[b]['kld_per_img']:.2f}\", f\"{beta_tests[b]['total_per_img']:.2f}\"] for b in betas]\n",
    "print_table(rows, [\"setting\", \"recon/img\", \"kld/img\", \"total/img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _ = next(iter(test_loader))\n",
    "x0 = x0[:1]\n",
    "dims = [0, 3, 7, 12, 20]\n",
    "\n",
    "for b in betas:\n",
    "    grids = latent_traversal(beta_models[b], x0, dims=dims, steps=7, span=3.0)\n",
    "    merged = torch.cat(grids, dim=0)  # (5*7, 1, 28, 28)\n",
    "    to_img_grid(merged, 5, 7, figsize=(10,7), title=f\"beta={b} | latent traversal (dims={dims}, span=-3..+3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Image generation with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we define our Conditional VAE.\n",
    "\n",
    "`one_hot` is a simple function that makes a vector with size `num_classes` which is zero everywhere but in `y`th element which is one.\n",
    "\n",
    "our model is a MLP VAE which concats vector in latent space with one-hot encoding of its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes=10):\n",
    "    y = y.long()\n",
    "    oh = torch.zeros(y.size(0), num_classes, device=y.device)\n",
    "    oh.scatter_(1, y.view(-1,1), 1.0)\n",
    "    return oh\n",
    "\n",
    "class MLPcVAE(nn.Module):\n",
    "    def __init__(self, z_dim=20, h_dim=512, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(28*28 + num_classes, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu = nn.Linear(h_dim, z_dim)\n",
    "        self.logvar = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(z_dim + num_classes, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        yoh = one_hot(y, self.num_classes)\n",
    "        h = self.enc(torch.cat([x, yoh], dim=1))\n",
    "        mu = self.mu(h)\n",
    "        logvar = self.logvar(h)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        x_hat = self.dec(torch.cat([z, yoh], dim=1)).view(-1, 1, 28, 28)\n",
    "        return {\"x_hat\": x_hat, \"mu\": mu, \"logvar\": logvar, \"z\": z}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_cvae(model, loader, beta=1.0):\n",
    "    model.eval()\n",
    "    rec_sum, kld_sum, total_sum, n = 0.0, 0.0, 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x, y)\n",
    "        x_hat, mu, logvar = out[\"x_hat\"], out[\"mu\"], out[\"logvar\"]\n",
    "        rec = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        total = rec + beta * kld\n",
    "        rec_sum += rec.item()\n",
    "        kld_sum += kld.item()\n",
    "        total_sum += total.item()\n",
    "        n += x.size(0)\n",
    "    return {\"recon_per_img\": rec_sum/n, \"kld_per_img\": kld_sum/n, \"total_per_img\": total_sum/n}\n",
    "\n",
    "def train_cvae(model, train_loader, val_loader, epochs=10, lr=2e-3, beta=1.0, warmup_epochs=0):\n",
    "    model.to(device)\n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        rec_sum, kld_sum, tot_sum, n = 0.0, 0.0, 0.0, 0\n",
    "        beta_ep = beta * min(1.0, ep / warmup_epochs) if warmup_epochs > 0 else beta\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x, y)\n",
    "            x_hat, mu, logvar = out[\"x_hat\"], out[\"mu\"], out[\"logvar\"]\n",
    "            rec = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "            kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = rec + beta_ep * kld\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            rec_sum += rec.item()\n",
    "            kld_sum += kld.item()\n",
    "            tot_sum += loss.item()\n",
    "            n += x.size(0)\n",
    "        va = evaluate_cvae(model, val_loader, beta=beta)\n",
    "        print(f\"ep {ep:02d} | beta={beta_ep:.3f} | train total {tot_sum/n:.2f} rec {rec_sum/n:.2f} kld {kld_sum/n:.2f} | val total {va['total_per_img']:.2f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = MLPcVAE(z_dim=20, h_dim=512).to(device)\n",
    "_ = train_cvae(cvae, train_loader, val_loader, epochs=12, lr=2e-3, beta=1.0, warmup_epochs=6)\n",
    "cvae_test = evaluate_cvae(cvae, test_loader, beta=1.0)\n",
    "cvae_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are making 20 images with random latent space for each valid label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cvae_generate(model, labels, n_per_label=20):\n",
    "    model.eval()\n",
    "    ys = torch.tensor(labels, device=device).repeat_interleave(n_per_label)\n",
    "    z = torch.randn(len(labels)*n_per_label, model.z_dim, device=device)\n",
    "    yoh = one_hot(ys, 10)\n",
    "    x_hat = model.dec(torch.cat([z, yoh], dim=1)).view(-1, 1, 28, 28)\n",
    "    return x_hat, ys\n",
    "\n",
    "# generate 20 per class (grid per class)\n",
    "imgs, ys = cvae_generate(cvae, labels=list(range(10)), n_per_label=20)\n",
    "plt.figure(figsize=(20,10))\n",
    "idx = 0\n",
    "for r in range(10):\n",
    "    for c in range(20):\n",
    "        plt.subplot(10,20,idx+1)\n",
    "        plt.imshow(imgs[idx].squeeze(0).cpu(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        idx += 1\n",
    "plt.suptitle('CVAE: 20 samples per class (rows=classes 0..9)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier for controllability and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class FashionResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        m = resnet18(weights=None)\n",
    "\n",
    "        m.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        m.maxpool = nn.Identity()\n",
    "\n",
    "        m.fc = nn.Identity()\n",
    "        self.backbone = m\n",
    "        self.head = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        feats = self.backbone(x)\n",
    "        logits = self.head(feats)\n",
    "        return logits, feats\n",
    "\n",
    "clf = FashionResNet18(num_classes=10).to(device)\n",
    "\n",
    "ckpt = torch.load(\"./classifier/fashion_resnet18_classifier.pt\", map_location=device)\n",
    "state = ckpt[\"model_state_dict\"] if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt else ckpt\n",
    "mean, std = float(ckpt[\"mean\"][0]), float(ckpt[\"std\"][0])\n",
    "\n",
    "clf.load_state_dict(state, strict=True)\n",
    "\n",
    "clf.eval()\n",
    "for p in clf.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clf_acc_on_loader(clf, loader):\n",
    "    clf.eval()\n",
    "    corr, n = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, _ = clf(x)\n",
    "        corr += (logits.argmax(1) == y).sum().item()\n",
    "        n += y.numel()\n",
    "    return corr / n\n",
    "\n",
    "clf_acc_on_loader(clf, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating with the given classifier, for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def norm_for_clf(x):\n",
    "    return (x - mean) / std\n",
    "\n",
    "@torch.no_grad()\n",
    "def cvae_accuracy_on_generated(model_cvae, clf, n_per_class=200):\n",
    "    model_cvae.eval(); clf.eval()\n",
    "    accs = []\n",
    "    for k in range(10):\n",
    "        imgs, ys = cvae_generate(model_cvae, labels=[k], n_per_label=n_per_class)\n",
    "        x = norm_for_clf(imgs.to(device))\n",
    "        logits, _ = clf(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        accs.append((pred == ys.to(device)).float().mean().item())\n",
    "    return accs, float(np.mean(accs))\n",
    "\n",
    "accs, acc_mean = cvae_accuracy_on_generated(cvae, clf, n_per_class=200)\n",
    "accs, acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.bar(np.arange(10), accs)\n",
    "plt.xticks(np.arange(10), classes, rotation=45, ha='right')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(f'Classifier accuracy on CVAE generated images (mean={acc_mean:.3f})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID using classifier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_np(x):\n",
    "    x = x - x.mean(axis=0, keepdims=True)\n",
    "    return (x.T @ x) / (x.shape[0] - 1)\n",
    "\n",
    "def sqrtm_psd(A, eps=1e-9):\n",
    "    A = 0.5 * (A + A.T)\n",
    "    w, V = np.linalg.eigh(A)\n",
    "    w = np.clip(w, eps, None)\n",
    "    return (V * np.sqrt(w)) @ V.T\n",
    "\n",
    "def fid_from_features(f1, f2):\n",
    "    m1, m2 = f1.mean(axis=0), f2.mean(axis=0)\n",
    "    C1, C2 = cov_np(f1), cov_np(f2)\n",
    "    diff = m1 - m2\n",
    "    prod = C1 @ C2\n",
    "    covmean = sqrtm_psd(prod)\n",
    "    return float(diff @ diff + np.trace(C1 + C2 - 2 * covmean))\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(loader, clf, max_items=None):\n",
    "    clf.eval()\n",
    "    feats = []\n",
    "    seen = 0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        _, f = clf(x)\n",
    "        feats.append(f.cpu().numpy())\n",
    "        seen += x.size(0)\n",
    "        if max_items is not None and seen >= max_items:\n",
    "            break\n",
    "    feats = np.concatenate(feats, axis=0)\n",
    "    if max_items is not None:\n",
    "        feats = feats[:max_items]\n",
    "    return feats\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features_from_images(imgs, clf, batch=256):\n",
    "    clf.eval()\n",
    "    feats = []\n",
    "    for i in range(0, imgs.size(0), batch):\n",
    "        x = imgs[i:i+batch].to(device)\n",
    "        _, f = clf(x)\n",
    "        feats.append(f.cpu().numpy())\n",
    "    return np.concatenate(feats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10,000 images from improved VAE\n",
    "@torch.no_grad()\n",
    "def generate_uncond_10000(model, n=10000, batch=256):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for i in range(0, n, batch):\n",
    "        m = min(batch, n-i)\n",
    "        if isinstance(model, ConvVAE):\n",
    "            x_hat = sample_prior_conv(model, n=m)\n",
    "        else:\n",
    "            x_hat = sample_prior(model, n=m)\n",
    "        outs.append(x_hat.cpu())\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "gen_10k = generate_uncond_10000(vae_imp, n=10000, batch=256)\n",
    "gen_10k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_real = extract_features(test_loader, clf, max_items=10000)\n",
    "f_fake = extract_features_from_images(gen_10k, clf, batch=256)\n",
    "fid_proxy = fid_from_features(f_real, f_fake)\n",
    "fid_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Report\n",
    "\n",
    "> you can read `README.md` for more explaination of the process in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rows = [\n",
    "    [\"MLP VAE (base)\", f\"{base_test['recon_per_img']:.2f}\", f\"{base_test['kld_per_img']:.2f}\", f\"{base_test['total_per_img']:.2f}\", \"-\"],\n",
    "    [\"Conv VAE (imp)\", f\"{imp_test['recon_per_img']:.2f}\", f\"{imp_test['kld_per_img']:.2f}\", f\"{imp_test['total_per_img']:.2f}\", f\"{fid_proxy:.2f}\"],\n",
    "    [\"CVAE (label)\", f\"{cvae_test['recon_per_img']:.2f}\", f\"{cvae_test['kld_per_img']:.2f}\", f\"{cvae_test['total_per_img']:.2f}\", \"-\"],\n",
    "]\n",
    "print_table(final_rows, [\"model\", \"recon/img\", \"kld/img\", \"total/img\", \"FID proxy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
